{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266a03d5-ddef-4c2c-92ee-5d6aab466532",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ca3bf0-3874-411f-9bf4-4a51e20ee4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense\n",
    "from tensorflow.keras.layers import Dense, Flatten, Input\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense\n",
    "from tensorflow.keras.layers import LSTM, Bidirectional, Dense, Conv2D, MaxPooling2D, Reshape, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a7bbc9-fcf8-406b-805a-e7e2907becae",
   "metadata": {},
   "source": [
    "### NNs & DNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556475ed-0b98-4e19-a7ec-a7ee47b95417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the extracted dataset\n",
    "data_path = 'M:\\Dissertation\\mit-bih-arrhythmia-database-1.0.0-20240722T094228Z-001\\mit-bih-arrhythmia-database-1.0.0' \n",
    "\n",
    "# Function to load a record and preprocess\n",
    "def load_and_preprocess(record):\n",
    "    signal, fields = wfdb.rdsamp(os.path.join(data_path, record))\n",
    "    annotation = wfdb.rdann(os.path.join(data_path, record), 'atr')\n",
    "    \n",
    "    # Use only one channel (e.g., channel 0)\n",
    "    signal = signal[:, 0].reshape(-1, 1)\n",
    "    \n",
    "    # Normalize the signal\n",
    "    scaler = StandardScaler()\n",
    "    signal = scaler.fit_transform(signal)\n",
    "    \n",
    "    # Segment the signal\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(len(annotation.sample)):\n",
    "        if annotation.sample[i] - 99 > 0 and annotation.sample[i] + 160 < len(signal):\n",
    "            segments.append(signal[annotation.sample[i] - 99 : annotation.sample[i] + 161])\n",
    "            labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(segments), np.array(labels)\n",
    "\n",
    "# Function to load and preprocess all records in the dataset\n",
    "def load_and_preprocess_all_records(data_path):\n",
    "    all_segments = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for record in os.listdir(data_path):\n",
    "        if record.endswith('.dat'):\n",
    "            record_name = record[:-4]  # Remove the file extension\n",
    "            segments, labels = load_and_preprocess(record_name)\n",
    "            all_segments.append(segments)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all segments and labels\n",
    "    all_segments = np.vstack(all_segments)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    return all_segments, all_labels\n",
    "\n",
    "# Load and preprocess the entire dataset\n",
    "segments, labels = load_and_preprocess_all_records(data_path)\n",
    "\n",
    "# Filter out unwanted labels (keeping only certain labels, e.g., 'N', 'A', 'L', 'R', 'V')\n",
    "valid_labels = ['N', 'A', 'L', 'R', 'V']\n",
    "mask = np.isin(labels, valid_labels)\n",
    "segments = segments[mask]\n",
    "labels = labels[mask]\n",
    "\n",
    "# Reshape segments to fit the model's expected input shape\n",
    "segments = segments.reshape(segments.shape[0], segments.shape[1], 1)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Convert labels to one-hot encoding for MSE models\n",
    "labels_one_hot = to_categorical(labels_encoded)\n",
    "\n",
    "# Split the data into training, validation, and test sets (80%, 10%, 10%)\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(segments, labels_encoded, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# Separate one-hot encoded labels for MSE models\n",
    "y_train_one_hot = to_categorical(y_train)\n",
    "y_val_one_hot = to_categorical(y_val)\n",
    "y_test_one_hot = to_categorical(y_test)\n",
    "\n",
    "# Functions to create the NN models\n",
    "def create_nn_model(input_shape, activation, optimizer, loss):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Flatten(),\n",
    "        Dense(128, activation=activation),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Functions to create the DNN models\n",
    "def create_dnn_model(input_shape, layers, activation, optimizer, loss):\n",
    "    model = Sequential([\n",
    "        Input(shape=input_shape),\n",
    "        Flatten()\n",
    "    ])\n",
    "    for _ in range(layers):\n",
    "        model.add(Dense(128, activation=activation))\n",
    "    model.add(Dense(5, activation='softmax'))\n",
    "    model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Create NN models with specified configurations\n",
    "nn_models = [\n",
    "    create_nn_model((260, 1), 'sigmoid', 'sgd', 'mean_squared_error'),\n",
    "    create_nn_model((260, 1), 'relu', 'sgd', 'mean_squared_error'),\n",
    "    create_nn_model((260, 1), 'relu', 'adam', 'mean_squared_error'),\n",
    "    create_nn_model((260, 1), 'relu', 'adam', 'sparse_categorical_crossentropy')\n",
    "]\n",
    "nn_model_names = ['NN-1', 'NN-2', 'NN-3', 'NN-4']\n",
    "nn_epochs = [1000, 600, 10, 10]\n",
    "\n",
    "# Create DNN models with specified configurations\n",
    "dnn_models = [\n",
    "    create_dnn_model((260, 1), 2, 'relu', 'adam', 'sparse_categorical_crossentropy'),\n",
    "    create_dnn_model((260, 1), 3, 'relu', 'adam', 'sparse_categorical_crossentropy'),\n",
    "    create_dnn_model((260, 1), 5, 'relu', 'adam', 'sparse_categorical_crossentropy')\n",
    "]\n",
    "dnn_model_names = ['DNN-1', 'DNN-2', 'DNN-3']\n",
    "dnn_epochs = [10, 10, 10]\n",
    "\n",
    "# Train and evaluate each NN model\n",
    "for model, name, epochs in zip(nn_models, nn_model_names, nn_epochs):\n",
    "    print(f'Training {name}...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Select appropriate labels for the model\n",
    "    if name in ['NN-1', 'NN-2', 'NN-3']:\n",
    "        y_train_labels = y_train_one_hot\n",
    "        y_val_labels = y_val_one_hot\n",
    "        y_test_labels = y_test_one_hot\n",
    "    else:\n",
    "        y_train_labels = y_train\n",
    "        y_val_labels = y_val\n",
    "        y_test_labels = y_test\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train_labels, epochs=epochs, validation_data=(X_val, y_val_labels), batch_size=128)\n",
    "    \n",
    "    # Calculate the total training time\n",
    "    total_training_time = time.time() - start_time\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "    \n",
    "    # Compute the metrics\n",
    "    overall_accuracy = np.sum(y_pred_classes == y_test) / len(y_test)\n",
    "    overall_sensitivity = recall_score(y_test, y_pred_classes, average='macro') * 100\n",
    "    overall_specificity = (conf_matrix[0,0] / (conf_matrix[0,0] + conf_matrix[0,1])) * 100 if conf_matrix.shape[0] > 1 else 0\n",
    "    overall_precision = precision_score(y_test, y_pred_classes, average='macro') * 100\n",
    "    overall_fscore = f1_score(y_test, y_pred_classes, average='macro') * 100\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
    "    print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")\n",
    "    print(f\"Overall Sensitivity: {overall_sensitivity:.2f}%\")\n",
    "    print(f\"Overall Specificity: {overall_specificity:.2f}%\")\n",
    "    print(f\"Overall Precision: {overall_precision:.2f}%\")\n",
    "    print(f\"Overall F-Score: {overall_fscore:.2f}%\")\n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(5, 5))\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{name} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# Train and evaluate each DNN model\n",
    "for model, name, epochs in zip(dnn_models, dnn_model_names, dnn_epochs):\n",
    "    print(f'Training {name}...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=128)\n",
    "    \n",
    "    # Calculate the total training time\n",
    "    total_training_time = time.time() - start_time\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "    \n",
    "    # Compute the metrics\n",
    "    overall_accuracy = np.sum(y_pred_classes == y_test) / len(y_test)\n",
    "    overall_sensitivity = recall_score(y_test, y_pred_classes, average='macro') * 100\n",
    "    overall_specificity = (conf_matrix[0,0] / (conf_matrix[0,0] + conf_matrix[0,1])) * 100 if conf_matrix.shape[0] > 1 else 0\n",
    "    overall_precision = precision_score(y_test, y_pred_classes, average='macro') * 100\n",
    "    overall_fscore = f1_score(y_test, y_pred_classes, average='macro') * 100\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
    "    print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")\n",
    "    print(f\"Overall Sensitivity: {overall_sensitivity:.2f}%\")\n",
    "    print(f\"Overall Specificity: {overall_specificity:.2f}%\")\n",
    "    print(f\"Overall Precision: {overall_precision:.2f}%\")\n",
    "    print(f\"Overall F-Score: {overall_fscore:.2f}%\")\n",
    "    \n",
    "    # Plot training & validation loss values\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title(f'{name} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation accuracy values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title(f'{name} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8112f5b-6699-4e30-aac1-3d932df23f08",
   "metadata": {},
   "source": [
    "### CNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad224255-069c-491b-bdc3-ef5e00dab644",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the extracted dataset\n",
    "data_path = 'M:\\Dissertation\\mit-bih-arrhythmia-database-1.0.0-20240722T094228Z-001\\mit-bih-arrhythmia-database-1.0.0'\n",
    "\n",
    "# Function to load a record and preprocess\n",
    "def load_and_preprocess(record):\n",
    "    signal, fields = wfdb.rdsamp(os.path.join(data_path, record))\n",
    "    annotation = wfdb.rdann(os.path.join(data_path, record), 'atr')\n",
    "    \n",
    "    # Use only one channel (e.g., channel 0)\n",
    "    signal = signal[:, 0].reshape(-1, 1)\n",
    "    \n",
    "    # Normalize the signal\n",
    "    scaler = StandardScaler()\n",
    "    signal = scaler.fit_transform(signal)\n",
    "    \n",
    "    # Segment the signal\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(len(annotation.sample)):\n",
    "        if annotation.sample[i] - 99 > 0 and annotation.sample[i] + 160 < len(signal):\n",
    "            segments.append(signal[annotation.sample[i] - 99 : annotation.sample[i] + 161])\n",
    "            labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(segments), np.array(labels)\n",
    "\n",
    "# Function to load and preprocess all records in the dataset\n",
    "def load_and_preprocess_all_records(data_path):\n",
    "    all_segments = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for record in os.listdir(data_path):\n",
    "        if record.endswith('.dat'):\n",
    "            record_name = record[:-4]  # Remove the file extension\n",
    "            segments, labels = load_and_preprocess(record_name)\n",
    "            all_segments.append(segments)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all segments and labels\n",
    "    all_segments = np.vstack(all_segments)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    return all_segments, all_labels\n",
    "\n",
    "# Load and preprocess the entire dataset\n",
    "segments, labels = load_and_preprocess_all_records(data_path)\n",
    "\n",
    "# Filter out unwanted labels (keeping only certain labels, e.g., 'N', 'L', 'R', 'A', 'V')\n",
    "valid_labels = ['N', 'L', 'R', 'A', 'V']\n",
    "mask = np.isin(labels, valid_labels)\n",
    "segments = segments[mask]\n",
    "labels = labels[mask]\n",
    "\n",
    "# Reshape segments to fit the model's expected input shape\n",
    "segments = segments.reshape(segments.shape[0], segments.shape[1], 1, 1)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(segments, labels_encoded, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n",
    "\n",
    "# Functions to create the CNN models\n",
    "def create_cnn_1(input_shape):\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        Conv2D(32, (5, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Flatten(),\n",
    "        Dense(4096, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_cnn_2(input_shape):\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        Conv2D(32, (5, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Conv2D(64, (3, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Flatten(),\n",
    "        Dense(4032, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_cnn_3(input_shape):\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        Conv2D(32, (5, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Conv2D(64, (3, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Conv2D(128, (5, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Flatten(),\n",
    "        Dense(3712, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "def create_cnn_4(input_shape):\n",
    "    model = Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=input_shape),\n",
    "        Conv2D(32, (5, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Conv2D(64, (3, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Conv2D(128, (5, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Conv2D(256, (3, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Flatten(),\n",
    "        Dense(3328, activation='relu'),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(5, activation='softmax')\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "input_shape = (260, 1, 1)  # Adjust this based on the actual input shape\n",
    "\n",
    "# Creating the models\n",
    "models = [create_cnn_1(input_shape), create_cnn_2(input_shape), create_cnn_3(input_shape), create_cnn_4(input_shape)]\n",
    "model_names = ['CNN-1', 'CNN-2', 'CNN-3', 'CNN-4']\n",
    "\n",
    "# Compile the models\n",
    "for model in models:\n",
    "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train and evaluate each model\n",
    "performance_results = []\n",
    "\n",
    "for model, name in zip(models, model_names):\n",
    "    print(f'Training {name}...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val), batch_size=256)\n",
    "    \n",
    "    # Calculate the total training time\n",
    "    total_training_time = time.time() - start_time\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test, y_pred_classes)\n",
    "    \n",
    "    # Compute the metrics\n",
    "    overall_accuracy = np.sum(y_pred_classes == y_test) / len(y_test)\n",
    "    overall_sensitivity = recall_score(y_test, y_pred_classes, average='macro') * 100\n",
    "    overall_specificity = (conf_matrix[0,0] / (conf_matrix[0,0] + conf_matrix[0,1])) * 100 if conf_matrix.shape[0] > 1 else 0\n",
    "    overall_precision = precision_score(y_test, y_pred_classes, average='macro') * 100\n",
    "    overall_fscore = f1_score(y_test, y_pred_classes, average='macro') * 100\n",
    "    \n",
    "    # Store the results for comparison\n",
    "    performance_results.append({\n",
    "        'Model': name,\n",
    "        'Accuracy (%)': overall_accuracy * 100,\n",
    "        'Sensitivity (%)': overall_sensitivity,\n",
    "        'Specificity (%)': overall_specificity,\n",
    "        'Precision (%)': overall_precision,\n",
    "        'F1-Score (%)': overall_fscore,\n",
    "        'Training Time (s)': total_training_time\n",
    "    })\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
    "    print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")\n",
    "    print(f\"Overall Sensitivity: {overall_sensitivity:.2f}%\")\n",
    "    print(f\"Overall Specificity: {overall_specificity:.2f}%\")\n",
    "    print(f\"Overall Precision: {overall_precision:.2f}%\")\n",
    "    print(f\"Overall F-Score: {overall_fscore:.2f}%\")\n",
    "    \n",
    "    # Plot training & validation accuracy values (as percentage)\n",
    "    plt.figure(figsize=(14, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.array(history.history['accuracy']) * 100, 'o-')\n",
    "    plt.plot(np.array(history.history['val_accuracy']) * 100, 'o-')\n",
    "    plt.title(f'{name} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], 'o-')\n",
    "    plt.plot(history.history['val_loss'], 'o-')\n",
    "    plt.title(f'{name} Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(['Train', 'Val'], loc='upper left')\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(10, 7))\n",
    "    sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues', xticklabels=valid_labels, yticklabels=valid_labels)\n",
    "    plt.title(f'{name} Confusion Matrix')\n",
    "    plt.xlabel('Predicted Labels')\n",
    "    plt.ylabel('True Labels')\n",
    "    plt.show()\n",
    "\n",
    "# Convert the performance results to a DataFrame for easy comparison\n",
    "performance_df = pd.DataFrame(performance_results)\n",
    "\n",
    "# Print the comparison table\n",
    "print(\"Performance Comparison Table:\")\n",
    "print(performance_df)\n",
    "\n",
    "# Additional comparison between CNN-1 and CNN-4 for specific classes\n",
    "class_indices = [valid_labels.index(label) for label in ['A', 'L', 'N', 'R', 'V']]\n",
    "comparison_results = []\n",
    "\n",
    "for name, model in zip(['CNN-1', 'CNN-4'], [models[0], models[3]]):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    \n",
    "    # Calculate metrics for each class\n",
    "    class_comparison = {\n",
    "        'Model': name,\n",
    "        'APB': {\n",
    "            'Acc': np.sum((y_test == class_indices[0]) & (y_pred_classes == class_indices[0])) / np.sum(y_test == class_indices[0]) * 100,\n",
    "            'Sen': recall_score(y_test == class_indices[0], y_pred_classes == class_indices[0]) * 100,\n",
    "            'Spe': (conf_matrix[0,0] / (conf_matrix[0,0] + conf_matrix[0,1])) * 100,\n",
    "            'Prec': precision_score(y_test == class_indices[0], y_pred_classes == class_indices[0]) * 100,\n",
    "            'F-Score': f1_score(y_test == class_indices[0], y_pred_classes == class_indices[0]) * 100,\n",
    "        },\n",
    "        'LBBB': {\n",
    "            'Acc': np.sum((y_test == class_indices[1]) & (y_pred_classes == class_indices[1])) / np.sum(y_test == class_indices[1]) * 100,\n",
    "            'Sen': recall_score(y_test == class_indices[1], y_pred_classes == class_indices[1]) * 100,\n",
    "            'Spe': (conf_matrix[1,1] / (conf_matrix[1,1] + conf_matrix[1,0])) * 100,\n",
    "            'Prec': precision_score(y_test == class_indices[1], y_pred_classes == class_indices[1]) * 100,\n",
    "            'F-Score': f1_score(y_test == class_indices[1], y_pred_classes == class_indices[1]) * 100,\n",
    "        },\n",
    "        'N': {\n",
    "            'Acc': np.sum((y_test == class_indices[2]) & (y_pred_classes == class_indices[2])) / np.sum(y_test == class_indices[2]) * 100,\n",
    "            'Sen': recall_score(y_test == class_indices[2], y_pred_classes == class_indices[2]) * 100,\n",
    "            'Spe': (conf_matrix[2,2] / (conf_matrix[2,2] + conf_matrix[2,1])) * 100,\n",
    "            'Prec': precision_score(y_test == class_indices[2], y_pred_classes == class_indices[2]) * 100,\n",
    "            'F-Score': f1_score(y_test == class_indices[2], y_pred_classes == class_indices[2]) * 100,\n",
    "        },\n",
    "        'RBBB': {\n",
    "            'Acc': np.sum((y_test == class_indices[3]) & (y_pred_classes == class_indices[3])) / np.sum(y_test == class_indices[3]) * 100,\n",
    "            'Sen': recall_score(y_test == class_indices[3], y_pred_classes == class_indices[3]) * 100,\n",
    "            'Spe': (conf_matrix[3,3] / (conf_matrix[3,3] + conf_matrix[3,0])) * 100,\n",
    "            'Prec': precision_score(y_test == class_indices[3], y_pred_classes == class_indices[3]) * 100,\n",
    "            'F-Score': f1_score(y_test == class_indices[3], y_pred_classes == class_indices[3]) * 100,\n",
    "        },\n",
    "        'PVC': {\n",
    "            'Acc': np.sum((y_test == class_indices[4]) & (y_pred_classes == class_indices[4])) / np.sum(y_test == class_indices[4]) * 100,\n",
    "            'Sen': recall_score(y_test == class_indices[4], y_pred_classes == class_indices[4]) * 100,\n",
    "            'Spe': (conf_matrix[4,4] / (conf_matrix[4,4] + conf_matrix[4,0])) * 100,\n",
    "            'Prec': precision_score(y_test == class_indices[4], y_pred_classes == class_indices[4]) * 100,\n",
    "            'F-Score': f1_score(y_test == class_indices[4], y_pred_classes == class_indices[4]) * 100,\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    comparison_results.append(class_comparison)\n",
    "\n",
    "# Convert the comparison results to a DataFrame for easy display\n",
    "formatted_df = pd.DataFrame()\n",
    "\n",
    "for i, class_label in enumerate(['APB', 'LBBB', 'N', 'RBBB', 'PVC']):\n",
    "    for metric in ['Acc', 'Sen', 'Spe', 'Prec', 'F-Score']:\n",
    "        formatted_df[f'{class_label} {metric} (%)'] = [comparison_results[0][class_label][metric], comparison_results[1][class_label][metric]]\n",
    "\n",
    "formatted_df.index = ['CNN-1', 'CNN-4']\n",
    "\n",
    "print(\"Class Comparison Between CNN-1 and CNN-4:\")\n",
    "print(formatted_df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03552db1-4496-408a-a3bd-96330d2f5aa4",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1819062-f37e-4341-9f29-4bb37bc65b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the extracted dataset\n",
    "data_path = 'M:\\Dissertation\\mit-bih-arrhythmia-database-1.0.0-20240722T094228Z-001\\mit-bih-arrhythmia-database-1.0.0'\n",
    "\n",
    "# Function to load a record and preprocess\n",
    "def load_and_preprocess(record):\n",
    "    signal, fields = wfdb.rdsamp(os.path.join(data_path, record))\n",
    "    annotation = wfdb.rdann(os.path.join(data_path, record), 'atr')\n",
    "    \n",
    "    # Use only one channel (e.g., channel 0)\n",
    "    signal = signal[:, 0].reshape(-1, 1)\n",
    "    \n",
    "    # Segment the signal\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(len(annotation.sample)):\n",
    "        if annotation.sample[i] - 99 > 0 and annotation.sample[i] + 160 < len(signal):\n",
    "            segments.append(signal[annotation.sample[i] - 99 : annotation.sample[i] + 161])\n",
    "            labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(segments), np.array(labels)\n",
    "\n",
    "# Function to load and preprocess all records in the dataset\n",
    "def load_and_preprocess_all_records(data_path):\n",
    "    all_segments = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for record in os.listdir(data_path):\n",
    "        if record.endswith('.dat'):\n",
    "            record_name = record[:-4]  # Remove the file extension\n",
    "            segments, labels = load_and_preprocess(record_name)\n",
    "            all_segments.append(segments)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all segments and labels\n",
    "    all_segments = np.vstack(all_segments)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    return all_segments, all_labels\n",
    "\n",
    "# Load and preprocess the entire dataset\n",
    "segments, labels = load_and_preprocess_all_records(data_path)\n",
    "\n",
    "# Filter out unwanted labels (keeping only certain labels, e.g., 'N', 'L', 'R', 'A', 'V')\n",
    "valid_labels = ['N', 'L', 'R', 'A', 'V']\n",
    "mask = np.isin(labels, valid_labels)\n",
    "segments = segments[mask]\n",
    "labels = labels[mask]\n",
    "\n",
    "# Reshape segments to fit the model's expected input shape\n",
    "segments = segments.reshape(segments.shape[0], segments.shape[1], 1)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "labels_categorical = tf.keras.utils.to_categorical(labels_encoded)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(segments, labels_categorical, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.3, random_state=42)\n",
    "\n",
    "# 1. Vanilla LSTM Model (return_sequences=False)\n",
    "def create_vanilla_lstm(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(32, return_sequences=False, input_shape=input_shape),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 2. Vanilla LSTM Model (return_sequences=True)\n",
    "def create_vanilla_lstm_seq(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(32, return_sequences=True, input_shape=input_shape),\n",
    "        LSTM(32),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 3. Stacked LSTM Model\n",
    "def create_stacked_lstm(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(32, return_sequences=True, input_shape=input_shape),\n",
    "        LSTM(32),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 4. Stacked Bidirectional LSTM Model\n",
    "def create_stacked_blstm(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(32, return_sequences=True), input_shape=input_shape),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the input shape based on your data\n",
    "input_shape = (segments.shape[1], 1)  # (timesteps, features)\n",
    "num_classes = len(valid_labels)\n",
    "\n",
    "# Create the models\n",
    "vanilla_lstm_model = create_vanilla_lstm(input_shape, num_classes)\n",
    "vanilla_lstm_seq_model = create_vanilla_lstm_seq(input_shape, num_classes)\n",
    "stacked_lstm_model = create_stacked_lstm(input_shape, num_classes)\n",
    "stacked_blstm_model = create_stacked_blstm(input_shape, num_classes)\n",
    "\n",
    "# Define the models and epochs for training\n",
    "models = [\n",
    "    (vanilla_lstm_model, 'Vanilla LSTM', 25),\n",
    "    (vanilla_lstm_seq_model, 'Vanilla LSTM (return_sequences=True)', 12),\n",
    "    (stacked_lstm_model, 'Stacked LSTM', 12),\n",
    "    (stacked_blstm_model, 'Bidirectional LSTM', 10)\n",
    "]\n",
    "\n",
    "# Train the models and evaluate\n",
    "for model, name, epochs in models:\n",
    "    print(f'Training {name}...')\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Train the model\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=32)\n",
    "    \n",
    "    # Calculate the total training time\n",
    "    total_training_time = time.time() - start_time\n",
    "    \n",
    "    # Predict on the test data\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "    y_test_classes = np.argmax(y_test, axis=1)\n",
    "    \n",
    "    # Compute the confusion matrix\n",
    "    conf_matrix = confusion_matrix(y_test_classes, y_pred_classes)\n",
    "    \n",
    "    # Compute the metrics\n",
    "    overall_accuracy = np.sum(y_pred_classes == y_test_classes) / len(y_test_classes)\n",
    "    overall_sensitivity = recall_score(y_test_classes, y_pred_classes, average='macro') * 100\n",
    "    overall_specificity = (conf_matrix[0,0] / (conf_matrix[0,0] + conf_matrix[0,1])) * 100 if conf_matrix.shape[0] > 1 else 0\n",
    "    overall_precision = precision_score(y_test_classes, y_pred_classes, average='macro') * 100\n",
    "    overall_fscore = f1_score(y_test_classes, y_pred_classes, average='macro') * 100\n",
    "    \n",
    "    # Print the results\n",
    "    print(f\"Results for {name}:\")\n",
    "    print(f\"Total Training Time: {total_training_time:.2f} seconds\")\n",
    "    print(f\"Overall Accuracy: {overall_accuracy * 100:.2f}%\")\n",
    "    print(f\"Overall Sensitivity: {overall_sensitivity:.2f}%\")\n",
    "    print(f\"Overall Specificity: {overall_specificity:.2f}%\")\n",
    "    print(f\"Overall Precision: {overall_precision:.2f}%\")\n",
    "    print(f\"Overall F-Score: {overall_fscore:.2f}%\")\n",
    "    \n",
    "    # Plot training & validation accuracy values (as percentage)\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    plt.plot(np.array(history.history['accuracy']) * 100, 'o-')\n",
    "    plt.plot(np.array(history.history['val_accuracy']) * 100, 'o-')\n",
    "    plt.title(f'{name} Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend(['Training Accuracy', 'Validation Accuracy'], loc='lower right')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e697c089-55f4-4e4b-89a4-3097e75d05f8",
   "metadata": {},
   "source": [
    "### CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809e4fb0-0c62-4b27-8ff0-2b388491d029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the extracted dataset\n",
    "data_path = 'M:\\Dissertation\\mit-bih-arrhythmia-database-1.0.0-20240722T094228Z-001\\mit-bih-arrhythmia-database-1.0.0'\n",
    "\n",
    "# Function to load a record and preprocess\n",
    "def load_and_preprocess(record):\n",
    "    signal, fields = wfdb.rdsamp(os.path.join(data_path, record))\n",
    "    annotation = wfdb.rdann(os.path.join(data_path, record), 'atr')\n",
    "    \n",
    "    # Use only one channel (e.g., channel 0)\n",
    "    signal = signal[:, 0].reshape(-1, 1)\n",
    "    \n",
    "    # Segment the signal\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(len(annotation.sample)):\n",
    "        if annotation.sample[i] - 99 > 0 and annotation.sample[i] + 160 < len(signal):\n",
    "            segments.append(signal[annotation.sample[i] - 99 : annotation.sample[i] + 161])\n",
    "            labels.append(annotation.symbol[i])\n",
    "    \n",
    "    return np.array(segments), np.array(labels)\n",
    "\n",
    "# Function to load and preprocess all records in the dataset\n",
    "def load_and_preprocess_all_records(data_path):\n",
    "    all_segments = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for record in os.listdir(data_path):\n",
    "        if record.endswith('.dat'):\n",
    "            record_name = record[:-4]  # Remove the file extension\n",
    "            segments, labels = load_and_preprocess(record_name)\n",
    "            all_segments.append(segments)\n",
    "            all_labels.append(labels)\n",
    "    \n",
    "    # Concatenate all segments and labels\n",
    "    all_segments = np.vstack(all_segments)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    \n",
    "    return all_segments, all_labels\n",
    "\n",
    "# Load and preprocess the entire dataset\n",
    "segments, labels = load_and_preprocess_all_records(data_path)\n",
    "\n",
    "# Filter out unwanted labels (keeping only certain labels, e.g., 'N', 'L', 'R', 'A', 'V')\n",
    "valid_labels = ['N', 'L', 'R', 'A', 'V']\n",
    "mask = np.isin(labels, valid_labels)\n",
    "segments = segments[mask]\n",
    "labels = labels[mask]\n",
    "\n",
    "# Reshape segments to fit the model's expected input shape\n",
    "segments = segments.reshape(segments.shape[0], segments.shape[1], 1, 1)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "labels_categorical = tf.keras.utils.to_categorical(labels_encoded)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(segments, labels_categorical, test_size=0.3, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 1. Vanilla LSTM Model\n",
    "def create_vanilla_lstm(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(32, input_shape=input_shape),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 2. Stacked LSTM Model\n",
    "def create_stacked_lstm(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        LSTM(32, return_sequences=True, input_shape=input_shape),\n",
    "        LSTM(32),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 3. Stacked Bidirectional LSTM Model\n",
    "def create_stacked_blstm(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        Bidirectional(LSTM(32, return_sequences=True), input_shape=input_shape),\n",
    "        Bidirectional(LSTM(32)),\n",
    "        Dense(32, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 4. CNN-LSTM Model\n",
    "def create_cnn_lstm(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        # CNN part\n",
    "        Conv2D(32, (5, 1), activation='relu', input_shape=input_shape),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Conv2D(64, (3, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Conv2D(128, (5, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Conv2D(256, (3, 1), activation='relu'),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        \n",
    "        # Reshape for LSTM input\n",
    "        Reshape((13, 256)),  # Adjusting the shape for LSTM input\n",
    "        \n",
    "        # LSTM part\n",
    "        LSTM(32, return_sequences=False),\n",
    "        \n",
    "        # Dense layers\n",
    "        Dense(64, activation='relu'),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the input shape based on your data\n",
    "input_shape_lstm = (segments.shape[1], 1)  # (timesteps, features) for LSTM\n",
    "input_shape_cnn = (segments.shape[1], 1, 1)  # (timesteps, features, 1) for CNN\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(valid_labels)\n",
    "\n",
    "# Create the models\n",
    "vanilla_lstm_model = create_vanilla_lstm(input_shape_lstm, num_classes)\n",
    "stacked_lstm_model = create_stacked_lstm(input_shape_lstm, num_classes)\n",
    "stacked_blstm_model = create_stacked_blstm(input_shape_lstm, num_classes)\n",
    "cnn_lstm_model = create_cnn_lstm(input_shape_cnn, num_classes)\n",
    "\n",
    "# Train the models (using pre-trained models if available)\n",
    "# For demonstration, I'll just run the training loop again for each model.\n",
    "\n",
    "models = [\n",
    "    (vanilla_lstm_model, 'Vanilla LSTM', 25),\n",
    "    (stacked_lstm_model, 'Stacked LSTM', 12),\n",
    "    (stacked_blstm_model, 'Bidirectional LSTM', 10),\n",
    "    (cnn_lstm_model, 'CNN-LSTM', 14)\n",
    "]\n",
    "\n",
    "for model, name, epochs in models:\n",
    "    print(f'Training {name}...')\n",
    "    start_time = time.time()\n",
    "    model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=32)\n",
    "    total_training_time = time.time() - start_time\n",
    "    print(f\"Total Training Time for {name}: {total_training_time:.2f} seconds\")\n",
    "\n",
    "# Predict using all models\n",
    "y_pred_vanilla = vanilla_lstm_model.predict(X_test)\n",
    "y_pred_stacked = stacked_lstm_model.predict(X_test)\n",
    "y_pred_bidirectional = stacked_blstm_model.predict(X_test)\n",
    "y_pred_cnn_lstm = cnn_lstm_model.predict(X_test)\n",
    "\n",
    "# Ensemble method - Average predictions\n",
    "y_pred_ensemble = (y_pred_vanilla + y_pred_stacked + y_pred_bidirectional + y_pred_cnn_lstm) / 4\n",
    "y_pred_classes_ensemble = np.argmax(y_pred_ensemble, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix_ensemble = confusion_matrix(y_test_classes, y_pred_classes_ensemble)\n",
    "\n",
    "# Compute the metrics\n",
    "overall_accuracy_ensemble = np.sum(y_pred_classes_ensemble == y_test_classes) / len(y_test_classes)\n",
    "overall_sensitivity_ensemble = recall_score(y_test_classes, y_pred_classes_ensemble, average='macro') * 100\n",
    "overall_specificity_ensemble = (conf_matrix_ensemble[0,0] / (conf_matrix_ensemble[0,0] + conf_matrix_ensemble[0,1])) * 100 if conf_matrix_ensemble.shape[0] > 1 else 0\n",
    "overall_precision_ensemble = precision_score(y_test_classes, y_pred_classes_ensemble, average='macro') * 100\n",
    "overall_fscore_ensemble = f1_score(y_test_classes, y_pred_classes_ensemble, average='macro') * 100\n",
    "\n",
    "# Print the ensemble results\n",
    "print(f\"Results for Ensemble of All Models:\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy_ensemble * 100:.2f}%\")\n",
    "print(f\"Overall Sensitivity: {overall_sensitivity_ensemble:.2f}%\")\n",
    "print(f\"Overall Specificity: {overall_specificity_ensemble:.2f}%\")\n",
    "print(f\"Overall Precision: {overall_precision_ensemble:.2f}%\")\n",
    "print(f\"Overall F-Score: {overall_fscore_ensemble:.2f}%\")\n",
    "\n",
    "# Calculate per-class metrics\n",
    "class_names = ['APB', 'LBBB', 'NSR', 'RBBB', 'PVC']\n",
    "accuracy_per_class_ensemble = []\n",
    "sensitivity_per_class_ensemble = []\n",
    "specificity_per_class_ensemble = []\n",
    "precision_per_class_ensemble = []\n",
    "f1score_per_class_ensemble = []\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    true_positive = conf_matrix_ensemble[i, i]\n",
    "    false_positive = np.sum(conf_matrix_ensemble[:, i]) - true_positive\n",
    "    false_negative = np.sum(conf_matrix_ensemble[i, :]) - true_positive\n",
    "    true_negative = np.sum(conf_matrix_ensemble) - (true_positive + false_positive + false_negative)\n",
    "    \n",
    "    accuracy = (true_positive + true_negative) / np.sum(conf_matrix_ensemble) * 100\n",
    "    sensitivity = true_positive / (true_positive + false_negative) * 100 if (true_positive + false_negative) > 0 else 0\n",
    "    specificity = true_negative / (true_negative + false_positive) * 100 if (true_negative + false_positive) > 0 else 0\n",
    "    precision = true_positive / (true_positive + false_positive) * 100 if (true_positive + false_positive) > 0 else 0\n",
    "    f1score = (2 * precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "    \n",
    "    accuracy_per_class_ensemble.append(accuracy)\n",
    "    sensitivity_per_class_ensemble.append(sensitivity)\n",
    "    specificity_per_class_ensemble.append(specificity)\n",
    "    precision_per_class_ensemble.append(precision)\n",
    "    f1score_per_class_ensemble.append(f1score)\n",
    "\n",
    "# Display the ensemble results in a DataFrame\n",
    "performance_df_ensemble = pd.DataFrame({\n",
    "    'Classes': class_names,\n",
    "    'Accuracy (%)': accuracy_per_class_ensemble,\n",
    "    'Sensitivity (%)': sensitivity_per_class_ensemble,\n",
    "    'Specificity (%)': specificity_per_class_ensemble,\n",
    "    'Precision (%)': precision_per_class_ensemble,\n",
    "    'F1 Score (%)': f1score_per_class_ensemble\n",
    "})\n",
    "\n",
    "# Calculate overall accuracy to match the table\n",
    "performance_df_ensemble['Overall Accuracy (%)'] = overall_accuracy_ensemble * 100\n",
    "\n",
    "print(performance_df_ensemble)\n",
    "\n",
    "# Plot ensemble confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_ensemble, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Ensemble Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c037fdf9-61c0-4d59-a5c4-638acfc8b6e9",
   "metadata": {},
   "source": [
    "### Stacked GRU with CNN-LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9623976-6d7f-4a71-bc59-08051e2cb685",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import wfdb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, GRU, Reshape, Input, Flatten, LSTM, Dropout\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Define the path to the extracted dataset\n",
    "data_path = 'M:\\\\Dissertation\\\\New folder\\\\mit-bih-arrhythmia-database-1.0.0'\n",
    "\n",
    "# Function to load a record and preprocess\n",
    "def load_and_preprocess(record):\n",
    "    signal, fields = wfdb.rdsamp(os.path.join(data_path, record))\n",
    "    annotation = wfdb.rdann(os.path.join(data_path, record), 'atr')\n",
    "   \n",
    "    # Use only one channel (e.g., channel 0)\n",
    "    signal = signal[:, 0].reshape(-1, 1)\n",
    "   \n",
    "    # Segment the signal\n",
    "    segments = []\n",
    "    labels = []\n",
    "    for i in range(len(annotation.sample)):\n",
    "        if annotation.sample[i] - 99 > 0 and annotation.sample[i] + 160 < len(signal):\n",
    "            segments.append(signal[annotation.sample[i] - 99 : annotation.sample[i] + 161])\n",
    "            labels.append(annotation.symbol[i])\n",
    "   \n",
    "    return np.array(segments), np.array(labels)\n",
    "\n",
    "# Function to load and preprocess all records in the dataset\n",
    "def load_and_preprocess_all_records(data_path):\n",
    "    all_segments = []\n",
    "    all_labels = []\n",
    "   \n",
    "    for record in os.listdir(data_path):\n",
    "        if record.endswith('.dat'):\n",
    "            record_name = record[:-4]  # Remove the file extension\n",
    "            segments, labels = load_and_preprocess(record_name)\n",
    "            all_segments.append(segments)\n",
    "            all_labels.append(labels)\n",
    "   \n",
    "    # Concatenate all segments and labels\n",
    "    all_segments = np.vstack(all_segments)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "   \n",
    "    return all_segments, all_labels\n",
    "\n",
    "# Load and preprocess the entire dataset\n",
    "segments, labels = load_and_preprocess_all_records(data_path)\n",
    "\n",
    "# Filter out unwanted labels (keeping only certain labels, e.g., 'N', 'L', 'R', 'A', 'V')\n",
    "valid_labels = ['N', 'L', 'R', 'A', 'V']\n",
    "mask = np.isin(labels, valid_labels)\n",
    "segments = segments[mask]\n",
    "labels = labels[mask]\n",
    "\n",
    "# Reshape segments to fit the model's expected input shape\n",
    "segments = segments.reshape(segments.shape[0], segments.shape[1], 1, 1)\n",
    "\n",
    "# Encode the labels\n",
    "label_encoder = LabelEncoder()\n",
    "labels_encoded = label_encoder.fit_transform(labels)\n",
    "\n",
    "# Convert labels to categorical format\n",
    "labels_categorical = tf.keras.utils.to_categorical(labels_encoded)\n",
    "\n",
    "# Split the data into training, validation, and test sets\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(segments, labels_categorical, test_size=0.2, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "\n",
    "# 1. Stacked GRU Model with Regularization\n",
    "def create_stacked_gru(input_shape, num_classes):\n",
    "    model = Sequential([\n",
    "        GRU(32, return_sequences=True, input_shape=input_shape),\n",
    "        GRU(32),\n",
    "        Dense(64, activation='relu'),\n",
    "        Dropout(0.5),\n",
    "        Dense(num_classes, activation='softmax')\n",
    "    ])\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# 2. CNN-LSTM Model with Regularization\n",
    "def create_cnn_lstm(input_shape_cnn, input_shape_lstm, num_classes):\n",
    "    cnn_model = Sequential([\n",
    "        Conv2D(32, (5, 1), activation='relu', input_shape=input_shape_cnn),\n",
    "        MaxPooling2D((2, 1)),\n",
    "        Conv2D(64, (3, 1), activation='relu'),\n",
    "        Flatten(),\n",
    "        Dense(260 * 32, activation='relu')  # Adjusted to match the LSTM input\n",
    "    ])\n",
    "   \n",
    "    cnn_input = Input(shape=input_shape_cnn)\n",
    "    cnn_output = cnn_model(cnn_input)\n",
    "   \n",
    "    # Correctly reshape the CNN output to match LSTM input expectations\n",
    "    timesteps = input_shape_lstm[0]  # This is typically the length of the input sequence\n",
    "    features = 32  # Reduced the features to match the Dense layer output\n",
    "   \n",
    "    lstm_input = Reshape((timesteps, features))(cnn_output)\n",
    "    lstm_output = LSTM(32)(lstm_input)\n",
    "   \n",
    "    output = Dense(num_classes, activation='softmax')(lstm_output)\n",
    "   \n",
    "    model = Model(inputs=cnn_input, outputs=output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Define the input shape based on your data\n",
    "input_shape_lstm_gru = (segments.shape[1], 1)  # (timesteps, features) for LSTM/GRU\n",
    "input_shape_cnn_resnet = (segments.shape[1], 1, 1)  # (timesteps, features, 1) for CNN/ResNet\n",
    "\n",
    "# Number of classes\n",
    "num_classes = len(valid_labels)\n",
    "\n",
    "# Create the models\n",
    "stacked_gru_model = create_stacked_gru(input_shape_lstm_gru, num_classes)\n",
    "cnn_lstm_model = create_cnn_lstm(input_shape_cnn_resnet, input_shape_lstm_gru, num_classes)\n",
    "\n",
    "# Train the models and store history\n",
    "models = [\n",
    "    (stacked_gru_model, 'Stacked GRU', 40),\n",
    "    (cnn_lstm_model, 'CNN-LSTM', 40)\n",
    "]\n",
    "\n",
    "histories = {}\n",
    "\n",
    "for model, name, epochs in models:\n",
    "    print(f'Training {name}...')\n",
    "    start_time = time.time()\n",
    "    history = model.fit(X_train, y_train, epochs=epochs, validation_data=(X_val, y_val), batch_size=32)\n",
    "    total_training_time = time.time() - start_time\n",
    "    print(f\"Total Training Time for {name}: {total_training_time:.2f} seconds\")\n",
    "    histories[name] = history\n",
    "\n",
    "# Predict using the models\n",
    "y_pred_stacked_gru = stacked_gru_model.predict(X_test)\n",
    "y_pred_cnn_lstm = cnn_lstm_model.predict(X_test)\n",
    "\n",
    "# Ensemble method - Average predictions from the models\n",
    "y_pred_ensemble = (y_pred_stacked_gru + y_pred_cnn_lstm) / 2\n",
    "y_pred_classes_ensemble = np.argmax(y_pred_ensemble, axis=1)\n",
    "y_test_classes = np.argmax(y_test, axis=1)\n",
    "\n",
    "# Compute the confusion matrix\n",
    "conf_matrix_ensemble = confusion_matrix(y_test_classes, y_pred_classes_ensemble)\n",
    "\n",
    "# Compute the metrics\n",
    "overall_accuracy_ensemble = np.sum(y_pred_classes_ensemble == y_test_classes) / len(y_test_classes)\n",
    "overall_sensitivity_ensemble = recall_score(y_test_classes, y_pred_classes_ensemble, average='macro') * 100\n",
    "overall_specificity_ensemble = (conf_matrix_ensemble[0,0] / (conf_matrix_ensemble[0,0] + conf_matrix_ensemble[0,1])) * 100 if conf_matrix_ensemble.shape[0] > 1 else 0\n",
    "overall_precision_ensemble = precision_score(y_test_classes, y_pred_classes_ensemble, average='macro') * 100\n",
    "overall_fscore_ensemble = f1_score(y_test_classes, y_pred_classes_ensemble, average='macro') * 100\n",
    "\n",
    "# Print the ensemble results\n",
    "print(f\"Results for Ensemble of Stacked GRU and CNN-LSTM Models:\")\n",
    "print(f\"Overall Accuracy: {overall_accuracy_ensemble * 100:.2f}%\")\n",
    "print(f\"Overall Sensitivity: {overall_sensitivity_ensemble:.2f}%\")\n",
    "print(f\"Overall Specificity: {overall_specificity_ensemble:.2f}%\")\n",
    "print(f\"Overall Precision: {overall_precision_ensemble:.2f}%\")\n",
    "print(f\"Overall F-Score: {overall_fscore_ensemble:.2f}%\")\n",
    "\n",
    "# Calculate per-class metrics\n",
    "class_names = label_encoder.inverse_transform(np.arange(num_classes))\n",
    "accuracy_per_class_ensemble = []\n",
    "sensitivity_per_class_ensemble = []\n",
    "specificity_per_class_ensemble = []\n",
    "precision_per_class_ensemble = []\n",
    "f1score_per_class_ensemble = []\n",
    "\n",
    "for i, class_name in enumerate(class_names):\n",
    "    true_positive = conf_matrix_ensemble[i, i]\n",
    "    false_positive = np.sum(conf_matrix_ensemble[:, i]) - true_positive\n",
    "    false_negative = np.sum(conf_matrix_ensemble[i, :]) - true_positive\n",
    "    true_negative = np.sum(conf_matrix_ensemble) - (true_positive + false_positive + false_negative)\n",
    "   \n",
    "    accuracy = (true_positive + true_negative) / np.sum(conf_matrix_ensemble) * 100\n",
    "    sensitivity = true_positive / (true_positive + false_negative) * 100 if (true_positive + false_negative) > 0 else 0\n",
    "    specificity = true_negative / (true_negative + false_positive) * 100 if (true_negative + false_positive) > 0 else 0\n",
    "    precision = true_positive / (true_positive + false_positive) * 100 if (true_positive + false_positive) > 0 else 0\n",
    "    f1score = (2 * precision * sensitivity) / (precision + sensitivity) if (precision + sensitivity) > 0 else 0\n",
    "   \n",
    "    accuracy_per_class_ensemble.append(accuracy)\n",
    "    sensitivity_per_class_ensemble.append(sensitivity)\n",
    "    specificity_per_class_ensemble.append(specificity)\n",
    "    precision_per_class_ensemble.append(precision)\n",
    "    f1score_per_class_ensemble.append(f1score)\n",
    "\n",
    "# Display the ensemble results in a DataFrame\n",
    "performance_df_ensemble = pd.DataFrame({\n",
    "    'Classes': class_names,\n",
    "    'Accuracy (%)': accuracy_per_class_ensemble,\n",
    "    'Sensitivity (%)': sensitivity_per_class_ensemble,\n",
    "    'Specificity (%)': specificity_per_class_ensemble,\n",
    "    'Precision (%)': precision_per_class_ensemble,\n",
    "    'F1 Score (%)': f1score_per_class_ensemble\n",
    "})\n",
    "\n",
    "# Add overall accuracy to match the table\n",
    "performance_df_ensemble['Overall Accuracy (%)'] = overall_accuracy_ensemble * 100\n",
    "\n",
    "print(performance_df_ensemble)\n",
    "\n",
    "# Plot ensemble confusion matrix\n",
    "plt.figure(figsize=(10, 7))\n",
    "sns.heatmap(conf_matrix_ensemble, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.title(f'Ensemble Confusion Matrix')\n",
    "plt.xlabel('Predicted Labels')\n",
    "plt.ylabel('True Labels')\n",
    "plt.show()\n",
    "\n",
    "# Plot accuracy and loss graphs for each model\n",
    "for name, history in histories.items():\n",
    "    plt.figure(figsize=(10, 5))\n",
    "   \n",
    "    # Plot accuracy\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(np.array(history.history['accuracy']) * 100, 'o--', label='Train Accuracy')\n",
    "    plt.plot(np.array(history.history['val_accuracy']) * 100, 'o--', label='Validation Accuracy')\n",
    "    plt.title(f'{name} Accuracy')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "   \n",
    "    # Plot loss\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history.history['loss'], 'o--', label='Train Loss')\n",
    "    plt.plot(history.history['val_loss'], 'o--', label='Validation Loss')\n",
    "    plt.title(f'{name} Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "   \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce845ba5-9405-41a7-b964-f5344d4bd1f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
